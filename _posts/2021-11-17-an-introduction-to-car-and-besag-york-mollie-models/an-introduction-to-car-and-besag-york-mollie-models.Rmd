---
title: "From CAR to Besag,York & Mollie Models"
description: | 
  Different approaches to modeling spatial areal networks within
  the conditional autoregressive family and a novel application.
author:
  - name: Adam Peterson
    url: https://apetersonsite.org
date: 01-16-2022
categories:
  - Statistics
  - Technical 
  - Cleanup vs. Crime
citation_url: https://xstreetvalidated.com
creative_commons: CC BY
bibliography: car.bib
output:
  distill::distill_article:
    self_contained: false
draft: false 
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(lubridate)
library(gt)
library(brms)
theme_set(theme_bw() + theme(text=element_text(size=22)))
```


Conditional Auto Regressive or CAR models are one of the foundational 
tools for modeling areal level data. In this post we'll examine some 
of the different variants of this model, the different strengths and 
weaknesses each possess and discuss how they can be used in modeling 
built environment data. This post is largely inspired by the 
[case study](https://mc-stan.org/users/documentation/case-studies/icar_stan.html#bym2-improving-the-parameterization-of-the-besag-york-and-mollie-model) by Mitzi et al. with less elaboration on 
technical details and a novel application to street crime data.

The first two sections focus on the technical details and history in the literature, 
while the third focuses on how this method can be applied 
and hints of how it might be used to inform urban policy making.

## The CAR model

The CAR family accommodates spatial correlation 
between spatially adjacent observations through the incorporation of a spatial 
effect $b_i$, whose covariance is a function of the spatial adjacency
between observations. Specifically, for an areal observation $y_i$, a linear model 
with covariate vector $\mathbf{x}_{i}$ and regression coefficients vector 
$\mathbf{\beta}$, would incorporate CAR effects as follows:

<aside>
The reader may recall this model family was first introduced in a  
[previous post](https://xstreetvalidated.com/posts/2021-08-26-spatial-models/) 
on spatial models.
</aside>

\begin{equation}
\begin{aligned}
y_i &=  \mathbf{x}^{T}_i\mathbf{\beta} + b_i\\
\mathbf{b} &\sim MVN(\mathbf{0},\Sigma)\\
\Sigma &= \tau D(I-\rho \mathbf{A}),
\end{aligned}
(\#eq:car)
\end{equation}

where $\Sigma$, the covariance matrix for $\mathbf{b}$ 
are a function of scale $\tau$, difference matrix^[where $D_{i,i}$ 
is the number of spatially adjacent neighbors for area $i$ and 0 everywhere 
else] $D$, adjacency matrix $\mathbf{A}$^[where $A_{i,j} = 1$ indicates area 
$i$ and $j$ are spatially connected and 0 indicates they are not], and spatial 
correlation $\rho$. When $\rho=1$ the adjacent observations are understood to 
be completely correlated and when fixing this parameter as such the model is 
called the Intrinsic Conditional Autoregressive (ICAR) model.


There are various theoretical details that make using CAR and ICAR models 
problematic. To start, when considering the spatial parameter $\rho$ unknown, 
as in the CAR framework, the parameter does not offer an intuitive 
interpretation; values of $\rho$ that are not very close to 1 allow for almost 
no spatial correlation whatsoever and negative spatial correlation usually does 
not comport with a substantive understanding of the data. As for the ICAR model, 
by fixing $\rho=1$, the correlation between observations is understood to be
completely driven by spatial association, which can be problematic in situations 
where there is non-spatial variability, or problems with over-dispersion, as is 
often in the case with spatial applications modeling counts using, e.g. a 
Poisson distribution. All of these issues prompted the 
development of a new approach by Besag, York and Mollie, that strives to take 
advantage of the inherent intuition behind the ICAR model but also overcome 
it's weaknesses. It is to this new framework we now turn our attention, but if 
the reader is interested in learning more about the details underlying the CAR 
and ICAR models they should consult chapter 3 of [@banerjee2003hierarchical], 
[@morris2019bayesian] and the latter's online 
[case study](https://mc-stan.org/users/documentation/case-studies/icar_stan.html).

## The Besag York and Mollie Model


The original paper by Besag York and Mollie (BYM) proposed a model that incorporated 
the two sources of variability ---  site specific and 
spatial variability --- previously addressed with one spatial effect term 
in ICAR models with two types of random effects: a site specific effect 
independent of spatial correlation, and a spatial effect complete with an ICAR 
variance structure [@besag1991bayesian]. 
The addition of this extra term caused *new* issues with identifiability and 
appropriate placement of priors which prompted further work by 
[@riebler2016intuitive] and others^[[@riebler2016intuitive] contains a 
nice summary of the work leading up to their model.] to further improve the 
parameterization introduced by BYM. By introducing a new constraint, 
shared scale $\sigma$ and correlation parameter $\rho$, their model improved 
identifiability and facilitated a more intuitive placement of priors on the 
hyper parameters. Denoting the scaled site-specific effect for the $i$th 
location as $\psi_i$ and the spatial effect as $\phi_i$ and their original 
variants as $\psi_i^{*}$,$\phi_i^{*}$, the new model specification is as follows:

\begin{equation}
\begin{aligned}
y_i &=  \mathbf{x}_{i}\mathbf{\beta} + b_i\\
\mathbf{b}_i  &= \psi_i + \phi_i \\
&= \sigma(\sqrt{1-\rho}\psi^{*}_i + \sqrt{\rho}\phi_i^{*}) \\
\psi^{*} &\sim N(0,\mathbf{I})\\
\phi^{*} &\sim N(0,\Sigma^{\star}) \\ 
\Sigma^{\star} &=  D(I-\rho \mathbf{A}).
\end{aligned}
(\#eq:bym2)
\end{equation}

Note that $\Sigma^{\star}$ is now scaled and constrained so that 
$V[\phi] \approx 1$. With all this machinery in place, the more application 
driven reader would be well warranted asking if **now**, we have a model 
that offers us a better chance of modeling spatially networked data. 
The short analysis in the following section aspires to show that we do.


## BYM Application to Crime and other City Street Events   

Turning our attention back to the class of crime data discussed in 
[@moyer2019effect] we look to see how we can improve the methods used in their 
work to better reflect the level at which more focused policy and community 
efforts can take effect: city streets.

The reader may recall that the authors used two approaches to estimate the 
effect of cleaned up vacant lots on the rate of violent shootings not involving
police officers in Philadelphia. The first used a kernel density estimate 
of the shootings, while the second used a simple count within a given buffer 
 --- 300 and 600 meters --- of the vacant lot. In this section I'd like to 
propose a method that will simplify the effort of "counting" where crimes 
occur, and enable a more intuitive modeling framework. Note that I do not (yet)
involve the estimation of the effect of vacant lots on the incidence of crime, 
leaving that topic for a future post.

### Mapping Crimes to Streets


The police reports documenting crime by the Philadelphia Police Department, 
contain a GPS coordinate as the location of the shooting, a single point in space 
and time, but individuals belonging to neighborhoods similarly understand the 
crime --- and many other similar city events --- to have occurred "on a street".
Thus the idea of mapping crimes from the single point location to the 
closest street by, e.g. euclidean distance^[Other distance measures could be 
used, but at the small distances we are considering the differences should be 
negligible.], and using the street as the spatial index where outcomes are 
measured follows naturally. This idea is illustrated in Figure 
\@ref(fig:sstreets), where shootings regularly occur on a city street 
or can be understood as occurring on a city street: if an event 
occurs inside a store whose address corresponds to a street, we would 
still colloquially understand the crime to have occurred "on the street", even 
though the crime did not occur on the asphalt representing the literal physical
street.


```{r data_prep}
uk <- read_sf("~/Documents/CityData/Philly/geo-data/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.shp") %>%
  filter(MAPNAME == "Upper Kensington")  %>% 
  st_transform(4326)

shootings <- read_csv("~/Documents/CityData/Philly/Crime/shootings_2015_2020.csv") %>% 
  filter(!is.na(point_x),!is.na(point_y)) %>% 
  filter(point_x > -80,
         year >= 2019,
         year <= 2021,
         officer_involved=="N") %>% 
  st_as_sf(coords = c("lng","lat")) %>% 
  st_set_crs(4326) %>% 
  st_filter(uk) %>% 
  mutate(Month = month(date_))

streets <- read_sf("~/Documents/CityData/Philly/CompleteStreets-shp/CompleteStreets.shp") %>% 
  st_filter(uk)

gddf <- shootings %>% 
  st_drop_geometry() %>% 
  distinct(year,Month) %>% 
  arrange(year,Month)

sdf <- map2_dfr(gddf$year,gddf$Month,function(x,y) {
  tibble(Year = x, 
         Month = y,
         street_ix = apply(st_distance(streets,
                                       shootings %>% 
                                         filter(year == {{x}},
                                                Month == {{y}})),
                           2, 
                           which.min), 
         count = 1) %>% 
    group_by(Year,Month,street_ix) %>% 
    count() %>% ungroup()
  })

mdf <- streets %>% 
  st_drop_geometry() %>% 
  mutate(street_ix = 1:n()) %>% 
  crossing(gddf) %>% 
  rename(Year = year) %>% 
  left_join(sdf) %>% 
  mutate(shootings = as.numeric(replace_na(n,0)),
         shooting = factor(as.integer((shootings>=1)*1))) %>% 
  select(Year,Month,shootings,shooting,OBJECTID) %>% 
  filter(OBJECTID!=34432) %>% ## singleton street
  mutate(Year = factor(Year),
         Month = factor(Month))

ntdf <- sfnetworks::as_sfnetwork(streets %>% 
                                   filter(OBJECTID!=34432),directed=F)

A <- ntdf %>% 
  tidygraph::convert(tidygraph::to_linegraph) %>% 
  igraph::as_adjacency_matrix()

rownames(A) <- streets %>% filter(OBJECTID != 34432) %>% pull(OBJECTID)
```

```{r sstreets, fig.width = 14, fig.height = 8, fig.cap = "Shootings in the Neighborhood of Upper Kensington during Calendar year 2015. City Street and Crime data are from opendataphilly.org."}
shootings %>% 
  ggplot() + 
  geom_sf(color='red') + 
  geom_sf(data=streets) + 
  theme_void()
```

Not only does mapping crimes to streets offer an intuitive understanding for 
modeling city events, such as the shootings studied in [@moyer2019effect], the 
street network also allows us to *borrow* information across the different streets
by street adjacency to model correlation through using, for example, 
the BYM parameterization of the ICAR prior. The previous modeling framework 
employed by [@moyer2019effect] used a neighborhood specific effect to try and
capture this correlation, but by incorporating 
spatial structure we should be able to improve upon their efforts to more
efficiently and accurately estimating the parameters of interest.
Figure \@ref(fig:networkvis) gives 
an illustration of how one might visualize the network determined by city 
streets.

```{r,networkvis, preview = TRUE, fig.cap="Network visualization of the streets in Upper Kensington. Note that this is not the exact manner in which the network information is used in the Besag York and Mollie model, as the streets are in fact considered to be the nodes of the graph and the street intersections the edges, but the plot is illustrative of the general idea."}
plot(ntdf)
```

### Zero Inflated Poisson BYM Model


The use of the BYM model is particularly important in modeling the count of 
shootings or other events on a particular street as we will need the 
extra flexibility provided by the random effect $\psi_i$, described above, 
to avoid problems with over-dispersion in modeling the counts as distributed 
from a poisson distribution. City events like shootings, accidents, etc. 
will typically by zero-inflated, as they do not --- typically --- occur 
at such a high frequency across every single street 
as to receive daily, monthly or possibly even an annual record. Thus, a 
zero point mass, or 
[zero-inflated model](https://en.wikipedia.org/wiki/Zero-inflated_model), is 
critical to accurately modeling the underlying process from which the data are 
generated. I'll present, fit and interpret a simple model to briefly
illustrate the idea.
Let $Y_{itm}$ be the number of shootings observed at street $i$, 
during calendar year $t$ and month $m$. Then let $\pi$ be the probability of an 
observation being drawn from the zero point mass. Using a similar notation as 
above for the BYM components, the model is as follows:

\begin{equation}
\begin{aligned}
Y_{itm} \sim \ \pi\delta_{0}(\cdot) +  
(1-\pi)\text{Pois}(\exp{(\alpha + \text{Year}_{t}\times \beta_t + 
\gamma_m + b_i)}),
\end{aligned}
(\#eq:ukeq)
\end{equation}

where $\delta_{0}(\cdot)$ is the dirac delta function at zero, $\alpha$ is a 
global intercept, $\beta_t$ is the difference between year $t$ and the 
reference year of 2019,$\gamma_m$ is a month specific effect, and $b_i$ is the 
composite spatial and street specific effect as described in \@ref(eq:bym2). 

We can fit this model in the `brms` package after creating the spatial network 
and adjacency matrix,`A`, from functions in the `sfnetworks`,`tidygraph` and 
`igraph` R packages[@sfnet;@igraph;@tgraph]. See the code in the 
[repo](https://github.com/apeterson91/xstreetvalidated) to learn more.

```{r bym_fit, echo = TRUE, cache = TRUE, message = FALSE, results = FALSE}
fit <- brm(shootings ~  Year + (1|Month) + car(A, gr = OBJECTID,type='bym2'),
           data = mdf, 
           data2 = list(A = A),
           family = zero_inflated_poisson(),
           cores = 3,
           chains = 3,
           backend = "cmdstanr",
           threads = threading(2),
           iter = 2E3)
```

<aside>
If you're wondering about the `brm` function and all the options used here 
I recommend starting with this paper [@buerkner] and then the vignette on 
[zero-inflated distributions](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html) 
and [within-chain parallelization](https://cran.r-project.org/web/packages/brms/vignettes/brms_threading.html)
to learn more.
</aside>

You can see the estimated monthly shooting rates by year in Table 
\@ref(tab:yrest) which are fairly similar and not of primary interest so I 
leave them in the details section below. Plotting the composite effects $b_i$ 
in Figure \@ref(fig:bym2plot) we're able to observe how the rate of shootings 
varies spatially across the city streets after adjusting for the global and 
local temporal effects captured by the other parameters, $\beta_t,\gamma_m$. 

```{r bym2plot, cache = TRUE, fig.cap = "Distribution of median monthly rate ratio deemed credible with 90% probability, as compared to a typical street, of shootings across city streets in Upper Kensington. Non credible effects are shown as 0."}
posterior_samples(fit) %>% 
  as_tibble() %>% 
  select(contains("rcar")) %>% 
  mutate(sample_ix = 1:n()) %>% 
  gather(contains("rcar"),key="Par",value="Sample") %>%
  mutate(street_ix = as.integer(str_extract(Par,"[0-9][0-9]?[0-9]?"))) %>% 
  select(street_ix,Sample,sample_ix) %>% 
  group_by(street_ix) %>% 
  summarise(`50%` = median(Sample),
            `5%` = quantile(Sample,0.05),
            `95%` = quantile(Sample,0.95)) %>% 
  ungroup() %>% 
  inner_join(streets %>% mutate(street_ix=1:n())) %>% 
  mutate(Credible_positive = (`5%` > 0 & `95%` > 0),
         Credible_negative = (`5%` < 0 & `95%` < 0)) %>% 
  mutate(Credible_display = Credible_positive*`50%` + Credible_negative*`50%`) %>% 
  # gather(contains("%"),key="Stat",value="StatValue") %>% 
  st_as_sf() %>% 
  ggplot() + 
  geom_sf(aes(color = Credible_display)) + 
  theme_void() + 
  scale_color_gradientn(colours = c("black","red")) + 
  #facet_grid(~Stat) + 
  theme(legend.title = element_blank())
```


The plot in \@ref(fig:bym2plot) shows the median log rate ratio of shootings 
for each street in the Upper Kensington Neighborhood that is determined to be "credibly"
non-zero. That is, with 90% probability the model estimates that the effect is 
greater than 0. We can interpret an example street estimate of 1.97 to indicate 
that that street has `exp(1.97)` $\approx$ 7 times more shootings than a typical 
street in Kensington (where $b_i = 0$), all else equal. Further, you 
should notice how this plot agrees with Figure \@ref(fig:sstreets) which is a 
nice reinforcement that the model is capturing the data correctly. There is 
much more that could be learned from this modeling framework. For example, 
the parameter $\pi$, could be made street specific, $\pi_{i}$, so as to identify
what features of a street are most likely to be associated with a complete 
lack of shootings. At this point, hopefully one can start to imagine how this 
model might be used in the context of urban policy decision making. By 
identifying those streets that are most at risk, the strength of correlation 
at the spatial or street specific level, or even just being able to offer a 
more intuitive interpretation of how policy may affect resident's lives in a 
context they can appreciate, there are lots of opportunities.
For the sake of brevity, I'll simply conclude with one final note, a reminder 
of what this model is and is **not** saying.

This model is merely descriptive --- it is describing 
the variability of shootings across the Upper Kensington neighborhood streets 
from 2019-2021 --- rather than offering us anything causal 
as [@moyer2019effect]'s work sought to do. This is however, as far as I can tell,
the first application of the BYM framework to city street level events^[
[@rose2018risk] attempted to do something similar using a Gaussian process].
I hope to show in a future post and possibly a proper paper, how this modeling 
framework can be combined with other ideas, such as the 
spatial temporal aggregated predictors I developed alongside my coauthors in 
[@rsstap;@peterson2021spatial] to improve the estimate of the vacant lot 
cleanup effect [@moyer2019effect] estimated in their work. Such efforts 
usually take a *long* time, but I will post more about that here as soon 
as any meaningful progress has been made. 


<details>
```{r yrest}
tbl <- fixef(fit,probs = c(.05,.95)) 
tbl[2,] <- tbl[1,] + tbl[2,]
tbl[3,] <- tbl[1,] + tbl[3,]
tbl %>% 
  as_tibble() %>% 
  select(-Est.Error) %>% 
  mutate_at(vars(everything()),function(x) round(exp(x),2)) %>% 
  mutate(Year = c("2019","2020","2021"),
         `Median (5%,95%)` = str_c(Estimate," (",Q5,", ",Q95,")")) %>% 
  select(Year,`Median (5%,95%)`) %>% 
  gt(caption="Estimate of street level monthly shootings rate by year.") 
```
</details>

```{r buffer_model,eval = FALSE}
df <- read_sf("~/Documents/CityData/Philly/vacant_lot_cleanups/vacant_lot_cleanups.shp") %>% 
  st_drop_geometry() %>% 
  mutate(lot_id = 1:n(),
         latlong = str_split(str_remove_all(location,"\\(|\\)"),", ")) %>% 
  mutate(Lat = map_dbl(latlong, function(x) as.numeric(x[[1]])),
         Long = map_dbl(latlong, function(x) as.numeric(x[[2]])) ) %>% 
  st_as_sf(.,coords = c("Long","Lat")) %>% 
  st_set_crs(4326) %>% 
  st_filter(uk)

rdf <- pltdf %>% 
  st_buffer(300) %>% 
  st_join(df %>% select(lot_id)) %>% 
  group_by(OBJECTID,shootings) %>% 
  summarise(num_lots = sum(!is.na(lot_id)))

dmat <- st_distance(pltdf,df %>% st_centroid()) / 1E3
attr(dmat,"units") <- NULL
class(dmat) <- setdiff(class(dmat),"units")

mdf$distances <- dmat

fit <- brm(bf(shootings ~ num_lots + car(A, gr = OBJECTID,type='bym2'), 
              zi ~ num_lots),
           data = mdf,
           data2 = list(A = A),
           family = zero_inflated_poisson(),
           cores = 4,
           chains = 3,
           backend = "cmdstanr",
           threads = threading(2),
           iter = 3E3)

```

