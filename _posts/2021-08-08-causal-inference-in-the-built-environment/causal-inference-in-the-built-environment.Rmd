---
title: "Causal Inference in the Built Environment"
description: |
  How can we know how to best change our environment?
author:
  - name: Adam Peterson
    url: https://apetersonsite.org
citation_url: https://xstreetvalidated.com
date: 08-08-2021
output:
  distill::distill_article:
    self_contained: false
categories:
  - Statistics
  - Technical
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


If we change the zoning on a region of land from being used exclusively
for single family houses to mixed-use apartments and retail, will this increase 
the number of cars on the nearby road? If an individual moves to an apartment
where they have a greater number of grocery store options to choose from, 
will they weigh less in 2 months than if they stayed at their current location?
These are the kinds of causal questions asked by those 
interested in understanding the built environment. I've written on this 
subject more broadly [elsewhere](https://www.apetersonsite.org/post/causal_inference_preface/), 
but want to return to it now in a setting where the complexities of 
working with built environment data can be more explicitly highlighted.
Note that if you're not familiar with causal inference concepts already you may
need to read the previous post series linked and/or check out one of the numerous
references I suggest [here](https://www.apetersonsite.org/post/phdlessons/).

# 1. Lack of Randomization

There are a number of obstacles that built environment data typically involve
when an investigator sets out to study causal relationship. The first
is that, typically, it is impossible to randomize the exposure - e.g. grocery stores -  of interest.
Whereas vaccines, drugs, and even some government programs can be randomly 
assigned to individuals, it isn't possible to randomly assign a park to a 
neighborhood or a certain kind of business to a specific storefront. 
These kinds of decisions are extremely expensive and will often involve a 
substantial amount of political decision making, rendering any attempt to 
introduce randomness a doomed prospect from the start.
As the reader may be aware, randomization is key to ensuring that the 
units of analysis - individuals, streets, etc. -  are comparable.
That is, there is no reason to think *a priori* that those who have been treated 
or exposed any different from those in the control. 

<aside>
If you want to see one interesting case where an environmental exposure
is randomized. See [here](https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2018.304752)
</aside>

This is obviously not the case in the built environment. We observe differences
in the underlying populations associated with certain built environment exposures
constantly. Dense, walk-able neighborhoods, tend to be much more expensive
than their suburban counterparts. Wealth is correlated with education, and 
so one might expect a wide range of different health outcomes between those in
dense walk-able neighborhoods for example and their more rural counterparts that
are not due to the built environment at all, but rather the wealth and/or education
with which they are correlated. Adjusting for these factors in order
to create as comparable a comparison as possible is crucial, if one is 
to trust the resulting causal estimate.


# 2. Interference

Even if one were able to randomize some treatment to a built environment 
feature^[any amenity or structure in the built environment, 
e.g. sidewalk availability or, in this case, vacant lots] (BEF), one then has to 
consider the effects of *interference*. Interference is the name given to the
phenomenon when an intervention or treatment on one entity affects the 
outcome of another. A violation of the commonly held Stable Unit 
Treatment Value Assumption or SUTVA, interference rears its heads often 
in the built environment when the treatment or intervention - the new
upzoned region -
affects not only the number of cars on the street immediately in front of it 
but also proximate streets, all under observation. Decomposing 
these effects across related units typically requires assumptions and/or 
prior understanding of the system under study. As such,
interference can represent a major obstacle to making progress in this space.

```{r,fig.width=14, fig.height = 8,fig.cap="A graph illustrating street connectivity and potential causal interference."}
library(tidygraph)
library(tidyverse)
library(ggraph)
theme_set(theme_bw() + 
            theme(text=element_text(size=22),
                  panel.border = element_blank()))
## use sample Philly street network graph here
nodes <- tibble(ID =c("Street_1","Street_2","Street_3","Street_4","Street_5",
           "Street_6","Street_7")) %>% 
  mutate(Label = str_replace(ID,"_"," "))
edges <- tibble(From = c("Street_1","Street_2","Street_3","Street_4","Street_5","Streeet_6","Street_2","Street_4"),
                To = c("Street_2","Street_3","Street_4","Street_5","Street_6","Street_7","Street_4","Street_6"))
gph <- tbl_graph(nodes = nodes,edges = edges,directed = F)
ggraph(gph,layout='stress') + geom_node_point() + geom_edge_link() + geom_node_label(aes(label = Label))
```

# 3. Homogeneity, Additivity and More

Still more problems await! In trying to understand how certain built environment
features may affect human health or other aspects of the built environment,
we'll likely have to make even further assumptions. In our example with grocery
stores, we'll likely need to make some assumption that say, Kroger and Meier
are equivalent grocery stores. This could be more justified than saying 
that the local farmer's market could also merit the label of a "grocery store". 
Assuming homogeneity in this manner greatly simplifies our modeling, but 
also runs us into conflicting with an assumption of 
[consistency](https://www.apetersonsite.org/post/causal_inference_preface/) at the heart 
of causal inference. 

Additivity is another such assumption that simplifies the mathematical tools required
to estimate the causal estimand^[an estimand is the thing which one wishes to estimate.]
of interest. However, just as with assuming homogeneity, the simplicity that 
additivity enables in our mathematical modeling may not be justified by our 
principled understanding of the causal system under study. 

Even now, after listing all these challenges I don't want to leave you with the 
impression that we've covered all the obstacles. Others may still rear their head! 

<aside>
Assuming additivity can turn the model $Y = f(x_{1},x_{2},...,x_{k})$
into $Y = \sum_k f(x_{k})$ which is considerably easier to estimate.
</aside>

# Solutions

At this point, it may seem as though there is no way we can ever hope 
to understand the built environment amidst all the many non-randomized, correlated
and interfering forces at work. Fear not, there are still *some* things we can do.
Though we'll likely never have the same level of confidence in our results 
as we would in a randomized control trial, we can try to incorporate more 
information through regression, narrow our focus with difference in 
difference estimators, model the interference associated with interventions, 
and allow for heterogeneous treatment effects in our model estimates.
All this and more will be discussed as a part of blog posts to come but if you're 
interested in learning more of the theory behind these ideas, I'd suggest you check out the statistics
references linked to on this site's reference page. Specifically the book by
Hernan and Robins entitled 
["Causal Inference - What if?"](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/), 
Angrist and Pitschke's work, 
["Mastering Metrics"](https://press.princeton.edu/books/paperback/9780691152844/mastering-metrics), 
["Counterfactuals and Causal 
Inference"](https://www.cambridge.org/core/books/counterfactuals-and-causal-inference/5CC81E6DF63C5E5A8B88F79D45E1D1B7) by Stephen Morgan and Christopher Winshop and 
["Regression and Other Stories"](https://avehtari.github.io/ROS-Examples/index.html)
by Gelman, Hill and Vehtari.


